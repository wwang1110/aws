{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37be34fc-0fa9-4811-865c-a3fdc38d38e8",
   "metadata": {},
   "source": [
    "# Fine-tune TinyLlama-1.1B for text-to-SQL generation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this workshop module, you will learn how to fine-tune a Llama-based LLM ([TinyLlama-1.1B-Chat-v1.0](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)) using causal language modelling so that the model learns how to generate SQL queries for text-based instructions. Your fine-tuning job will be launched using SageMaker Training which provides a serverless training environment where you do not need to manage the underlying infrastructure. You will learn how to configure a PyTorch training job using [SageMaker's PyTorch estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html), and how to leverage the [Hugging Face Optimum Neuron](https://github.com/huggingface/optimum-neuron) package to easily run the PyTorch training job with AWS Trainium accelerators via an [AWS EC2 trn1.2xlarge instance](https://aws.amazon.com/ec2/instance-types/trn1/).\n",
    "\n",
    "For this module, you will be using the [b-mc2/sql-create-context](https://huggingface.co/datasets/b-mc2/sql-create-context) dataset which consists of thousands of examples of SQL schemas, questions about the schemas, and SQL queries intended to answer the questions.\n",
    "\n",
    "*Dataset example 1:*\n",
    "* *SQL schema/context:* `CREATE TABLE management (department_id VARCHAR); CREATE TABLE department (department_id VARCHAR)`\n",
    "* *Question:* `How many departments are led by heads who are not mentioned?`\n",
    "* *SQL query/answer:* `SELECT COUNT(*) FROM department WHERE NOT department_id IN (SELECT department_id FROM management)`\n",
    "\n",
    "*Dataset example 2:*\n",
    "* *SQL schema/context:* `CREATE TABLE courses (course_name VARCHAR, course_id VARCHAR); CREATE TABLE student_course_registrations (student_id VARCHAR, course_id VARCHAR)`\n",
    "* *Question:* `What are the ids of all students for courses and what are the names of those courses?`\n",
    "* *SQL query/answer:* `SELECT T1.student_id, T2.course_name FROM student_course_registrations AS T1 JOIN courses AS T2 ON T1.course_id = T2.course_id`\n",
    "\n",
    "By fine-tuning the model over several thousand of these text-to-SQL examples, the model will then learn how to generate an appropriate SQL query when presented with a SQL context and a free-form question.\n",
    "\n",
    "This text-to-SQL use case was selected so you can successfully fine-tune your model in a reasonably short amount of time (~20 minutes) which is appropriate for this 1hr workshop. Although this is a relatively simple use case, please keep in mind that the same techniques and components used in this module can also be applied to fine-tune LLMs for more advanced use cases such as writing code, summarizing documents, creating blog posts - the possibilities are endless!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866074ee-c300-4793-8e63-adbcfc314ad8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "This notebook uses the SageMaker Python SDK to prepare, launch, and monitor the progress of a PyTorch-based training job. Before we get started, it is important to upgrade the SageMaker SDK to ensure that you are using the latest version. Run the next two cells to upgrade the SageMaker SDK and set up your session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3264aae2-1f18-4b59-a92c-2f169903c202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upgrade SageMaker SDK to the latest version\n",
    "%pip install -U sagemaker awscli -q 2>&1 | grep -v \"warnings/venv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5ed574-6db5-471b-8515-c0f6189e653e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "sagemaker_config_logger = logging.getLogger(\"sagemaker.config\")\n",
    "sagemaker_config_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Import SageMaker SDK, setup our session\n",
    "from sagemaker import get_execution_role, Session\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "sess = Session()\n",
    "default_bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4193108b-25fb-4d3e-85db-c66b8c04c251",
   "metadata": {},
   "source": [
    "## Specify the Optimum Neuron deep learning container (DLC) image\n",
    "\n",
    "The SageMaker Training service uses containers to execute your training script, allowing you to fully customize your training script environment and any required dependencies. For this workshop, you will use a recent Optimum Neuron deep learning container (DLC) image which is an AWS-maintained image containing the Neuron SDK, PyTorch, and the Hugging Face Optimum Neuron library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247ad886-6977-4295-947b-86d4892b48bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the Neuron DLC that we will use for training\n",
    "#   For now, we'll use the standard Neuron DLC and install Optimum Neuron v0.0.25 at training time. Let's replace this with new Optimum Neuron DLC once it's available.\n",
    "\n",
    "training_image = f\"763104351884.dkr.ecr.{sess.boto_region_name}.amazonaws.com/pytorch-training-neuronx:2.1.2-neuronx-py310-sdk2.20.1-ubuntu20.04\"\n",
    "print(training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8802bc-657a-419d-b86d-eb8af5eff90e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configure the PyTorch Estimator\n",
    "\n",
    "The SageMaker SDK includes a [PyTorch Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html) class which you can use to define a PyTorch training job that will be executed in the SageMaker managed environment. \n",
    "\n",
    "In the following cell, you will create a PyTorch Estimator which will run the attached `finetune_llama.py` training script on a trn1.2xlarge instance. The `finetune_llama.py` script is an Optimum Neuron training script that can be used for causal language modelling with AWS Trainium.\n",
    "\n",
    "The PyTorch Estimator has many parameters that can be used to configure your training job. A few of the most important parameters include:\n",
    "\n",
    "- *entry_point*: refers to the name of the training script that will be executed as part of this training job\n",
    "- *source_dir*: the path to the local source code directory (relative to your notebook) that will be packaged up and included inside your training container\n",
    "- *instance_count*: defines how many EC2 instances to use for this training job\n",
    "- *instance_type*: determines which type of EC2 instance will be used for training\n",
    "- *image_uri*: defines which training DLC will be used to run the training job (see Neuron DLC, above)\n",
    "- *distribution*: determines which type of distribution to use for the training job - you will need 'torch_distributed' for this workshop\n",
    "- *environment*: provides a dictionary of environment variables which will be applied to your training environment\n",
    "- *hyperparameters*: provides a dictionary of command-line arguments to pass to your training script, ex: finetune_llama.py\n",
    "\n",
    "In the `hyperparameters` section, you can see the specific command-line arguments that are used to control the behavior of the `finetune_llama.py` training script. Notably:\n",
    "- *model_id*: specifies which model you will be fine-tuning, in this case a recent checkpoint from the TinyLlama-1.1B project\n",
    "- *tokenizer_id*: specifies which tokenizer you will used to tokenize the dataset examples during training\n",
    "- *output_dir*: directory in which the fine-tuned model will be saved. Here we use the SageMaker-specific `/opt/ml/model` directory. At the end of the training job, SageMaker automatically copies the contents of this directory to S3 on your behalf\n",
    "- *tensor_parallel_size*: the tensor parallel degree for which we want to use for training. In this case we use '2' to shard the model across the 2 NeuronCores available in the trn1.2xlarge instance\n",
    "- *bf16*: request BFloat16 training\n",
    "- *per_device_train_batch_size*: the microbatch size to be used for fine-tuning\n",
    "- *gradient_accumulation_steps*: how many steps for which gradients will be accumulated between updates\n",
    "- *max_steps*: the maximum number of steps of fine-tuning that we want to perform\n",
    "- *lora_r*, *lora_alpha*, *lora_dropout*: the LoRA rank, alpha, and dropout values to use during fine-tuning\n",
    "\n",
    "The below estimator has been pre-configured for you, so you do not need to make any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e28014c-4d0b-452b-9bde-44aa10e61bb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the PyTorch estimator\n",
    "# Note that the hyperparameters are just command-line args passed to the finetune_llama.py script to control its behavior\n",
    "\n",
    "pt_estimator = PyTorch(\n",
    "        entry_point=\"finetune_llama.py\",\n",
    "        source_dir=\"./assets\",\n",
    "        role=get_execution_role(),\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.trn1.2xlarge\",\n",
    "        disable_profiler=True,\n",
    "        output_path=f\"s3://{default_bucket}/neuron_events2024\",\n",
    "        base_job_name=\"trn1-tinyllama\",\n",
    "        sagemaker_session=sess,\n",
    "        code_bucket=f\"s3://{default_bucket}/neuron_events2024_code\",\n",
    "        checkpoint_s3_uri=f\"s3://{default_bucket}/neuron_events_output\",\n",
    "        image_uri=training_image,\n",
    "        distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "        environment={\"FI_EFA_FORK_SAFE\": \"1\", \"WANDB_DISABLED\": \"true\"},\n",
    "        disable_output_compression=True,\n",
    "        hyperparameters={\n",
    "            \"model_id\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "            \"tokenizer_id\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "            \"output_dir\": \"/opt/ml/model\",\n",
    "            \"tensor_parallel_size\": 2,\n",
    "            \"bf16\": True,\n",
    "            \"per_device_train_batch_size\": 2,\n",
    "            \"gradient_accumulation_steps\": 1,\n",
    "            \"gradient_checkpointing\": True,\n",
    "            \"max_steps\": 1000,\n",
    "            \"lora_r\": 16,\n",
    "            \"lora_alpha\": 32,\n",
    "            \"lora_dropout\": 0.05,\n",
    "            \"logging_steps\": 10,\n",
    "            \"learning_rate\": 5e-5,\n",
    "            \"dataloader_drop_last\": True,\n",
    "            \"disable_tqdm\": True\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2278940b-f563-4582-9df0-bd56d9b5fd28",
   "metadata": {},
   "source": [
    "## Launch the training job\n",
    "\n",
    "Once the estimator has been created, you can then launch your training job by calling `.fit()` on the estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7829c64-0190-43c3-be1a-0ccce7d45248",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call fit() on the estimator to initiate the training job\n",
    "pt_estimator.fit(wait=False, logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77434b2-94d7-4256-8d0b-d5d2ddb1d5ae",
   "metadata": {},
   "source": [
    "## Monitor the training job\n",
    "\n",
    "When the training job has been launched, the SageMaker Training service will then take care of:\n",
    "- launching and configuring the requested EC2 infrastructure for your training job\n",
    "- launching the requested container image on each of the EC2 instances\n",
    "- copying your source code directory and running your training script within the container(s)\n",
    "- storing your trained model artifacts in Amazon Simple Storage Service (S3)\n",
    "- decommissioning the training infrastructure\n",
    "\n",
    "While the training job is running, the following cell will periodically check and output the job status. When you see 'Completed', you know that your training job is finished and you can proceed to the remainder of the notebook. The training job typically takes about 20 minutes to complete.\n",
    "\n",
    "If you are interested in viewing the output logs from your training job, you can view the logs by navigating to the AWS CloudWatch console, selecting `Logs -> Log Groups` in the left-hand menu, and then looking for your SageMaker training job in the list. **Note:** it will usually take 4-5 minutes before the infrastructure is running and the output logs begin to be populated in CloudWatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c223037-2f8e-4eb0-9e4b-ff4dac6ede7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Periodically check job status until it shows 'Completed' (ETA ~20 minutes)\n",
    "#  You can also monitor job status in the SageMaker console, and view the\n",
    "#  SageMaker Training job logs in the CloudWatch console\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "\n",
    "while (job_status := pt_estimator.jobs[-1].describe()['TrainingJobStatus']) not in ['Completed', 'Error']:\n",
    "    print(f\"{datetime.now().isoformat()} Training job status: {job_status}!\")\n",
    "    sleep(30)\n",
    "\n",
    "print(f\"\\n{datetime.now().isoformat()} Training job status: {job_status}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c94343-b0c6-4903-82cc-c8ab2f88b26b",
   "metadata": {},
   "source": [
    "## Determine location of fine-tuned model artifacts\n",
    "\n",
    "Once the training job has completed, SageMaker will copy your fine-tuned model artifacts to a specified location in S3.\n",
    "\n",
    "In the following cell, you can see how to programmatically determine the location of your model artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213af977-8ed6-4081-af65-59c70db2dbfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show where the fine-tuned model is stored - previous job must be 'Completed' before running this cell\n",
    "model_archive_path = pt_estimator.jobs[-1].describe()['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(f\"Your fine-tuned model is available here:\\n\\n{model_archive_path}/merged_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68f529f-a548-4fbd-b160-3cab5f52c488",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "**Note:** Please copy the above S3 path, as it will be required in the subsequent workshop module.\n",
    "\n",
    "\n",
    "Lastly, run the following cell to list the model artifacts available in your S3 model_archive_path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ad8c7e-6a73-4f20-944f-ac12ef286a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View the contents of the fine-tuned model path in S3\n",
    "!aws s3 ls {model_archive_path}/merged_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac87e73b-baa0-451a-9d90-91b3e4c6f83e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fca9ffa7-a694-48c0-acde-cd468d18a448",
   "metadata": {},
   "source": [
    "Congratulations on completing the LLM fine-tuning module!\n",
    "\n",
    "In the next notebook, you will learn how to deploy your fine-tuned model in a SageMaker hosted endpoint, and leverage AWS Inferentia accelerators to perform model inference. Have fun!"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
